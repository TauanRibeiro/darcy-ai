# Darcy AI - InstruÃ§Ãµes de InstalaÃ§Ã£o e Teste

## ğŸ“‹ PrÃ©-requisitos

1. **Node.js** (versÃ£o 16 ou superior)
2. **npm** (versÃ£o 8 ou superior)
3. **Git** (opcional, para clonar o repositÃ³rio)

### Para funcionalidades completas:
- **Ollama** (para LLM local) - [Instalar Ollama](https://ollama.ai)
- **FFmpeg** (para processamento de Ã¡udio) - [Instalar FFmpeg](https://ffmpeg.org/download.html)

## ğŸš€ InstalaÃ§Ã£o

### 1. Frontend
O frontend funciona diretamente no navegador. Abra `index.html` em qualquer navegador moderno ou use um servidor local:

```bash
# Usando Python (se disponÃ­vel)
python -m http.server 8000

# Usando Node.js (se disponÃ­vel)
npx serve .

# Usando Live Server no VS Code
# Instale a extensÃ£o "Live Server" e clique em "Go Live"
```

### 2. Backend (Opcional - para funcionalidades avanÃ§adas)

```bash
# 1. Navegue para a pasta do backend
cd backend

# 2. Instale as dependÃªncias
npm install

# 3. Configure as variÃ¡veis de ambiente
cp .env.example .env

# 4. Edite o arquivo .env com suas configuraÃ§Ãµes
# Use seu editor preferido para configurar as chaves de API
notepad .env  # Windows
# ou
code .env     # VS Code
```

### 3. ConfiguraÃ§Ã£o do arquivo .env

Edite o arquivo `backend/.env` com suas configuraÃ§Ãµes:

```env
# ConfiguraÃ§Ãµes do servidor
PORT=3001
NODE_ENV=development

# URLs dos LLM providers
OLLAMA_URL=http://localhost:11434
OPENAI_API_URL=https://api.openai.com/v1
ANTHROPIC_API_URL=https://api.anthropic.com/v1
COHERE_API_URL=https://api.cohere.ai/v1

# Chaves de API (opcional - configure apenas as que vocÃª vai usar)
OPENAI_API_KEY=sua_chave_openai_aqui
ANTHROPIC_API_KEY=sua_chave_anthropic_aqui
COHERE_API_KEY=sua_chave_cohere_aqui

# ConfiguraÃ§Ãµes de upload
MAX_FILE_SIZE=10485760
ALLOWED_FILE_TYPES=pdf,docx,txt,jpg,jpeg,png,gif,mp3,wav,m4a

# Rate limiting
RATE_LIMIT_WINDOW_MS=900000
RATE_LIMIT_MAX_REQUESTS=100
```

## â–¶ï¸ ExecuÃ§Ã£o

### Modo Frontend-Only (Funcionalidade bÃ¡sica)
1. Abra `index.html` no navegador
2. Configure um provider (Ollama local ou chaves de API na seÃ§Ã£o "ConfiguraÃ§Ãµes")
3. Comece a usar o Darcy AI!

### Modo Completo (Frontend + Backend)

```bash
# Terminal 1: Inicie o backend
cd backend
npm start
# ou para desenvolvimento com auto-reload:
npm run dev

# Terminal 2: Sirva o frontend (se necessÃ¡rio)
# Abra index.html no navegador ou use um servidor local
python -m http.server 8000
```

## ğŸ§ª Testes

### Teste Frontend
1. Abra `index.html` no navegador
2. VÃ¡ para "ConfiguraÃ§Ãµes" e configure um provider
3. Teste uma conversa simples
4. Teste upload de arquivo (se backend estiver rodando)
5. Teste busca na web

### Teste Backend
```bash
cd backend

# Teste de saÃºde da API
curl http://localhost:3001/api/health

# Teste de providers disponÃ­veis
curl http://localhost:3001/api/providers

# Executar testes automatizados (se configurados)
npm test
```

### Testes de Funcionalidades

#### 1. Teste de Chat BÃ¡sico
- Envie uma mensagem simples como "OlÃ¡, como vocÃª pode me ajudar?"
- Verifique se a resposta Ã© adequada

#### 2. Teste de Upload de Arquivo
- FaÃ§a upload de um arquivo PDF ou Word
- Verifique se o conteÃºdo Ã© processado corretamente

#### 3. Teste de Busca na Web
- Digite uma pergunta comeÃ§ando com "Buscar na web:"
- Verifique se os resultados sÃ£o incorporados na resposta

#### 4. Teste de Diferentes Providers
- Configure diferentes providers (Ollama, OpenAI, etc.)
- Teste a mudanÃ§a entre eles

## ğŸ”§ ConfiguraÃ§Ã£o AvanÃ§ada

### Ollama Local
```bash
# Instalar Ollama
curl -fsSL https://ollama.ai/install.sh | sh

# Baixar um modelo (exemplo: Llama 2)
ollama pull llama2

# Verificar modelos instalados
ollama list

# Testar Ollama
curl http://localhost:11434/api/generate \
  -d '{
    "model": "llama2",
    "prompt": "Why is the sky blue?",
    "stream": false
  }'
```

### FFmpeg (para processamento de Ã¡udio)
```bash
# Windows (usando Chocolatey)
choco install ffmpeg

# Windows (usando Scoop)
scoop install ffmpeg

# Linux (Ubuntu/Debian)
sudo apt update && sudo apt install ffmpeg

# macOS (usando Homebrew)
brew install ffmpeg
```

## ğŸ“Š Estrutura do Projeto

```
darcy/
â”œâ”€â”€ index.html              # Interface principal
â”œâ”€â”€ style.css              # Estilos principais
â”œâ”€â”€ script.js              # Script principal
â”œâ”€â”€ js/                    # MÃ³dulos JavaScript
â”‚   â”œâ”€â”€ config.js         # ConfiguraÃ§Ãµes
â”‚   â”œâ”€â”€ storage.js        # Gerenciamento de dados
â”‚   â”œâ”€â”€ llm-providers.js  # IntegraÃ§Ã£o com LLMs
â”‚   â”œâ”€â”€ chat-interface.js # Interface de chat
â”‚   â”œâ”€â”€ file-handler.js   # ManipulaÃ§Ã£o de arquivos
â”‚   â”œâ”€â”€ settings-panel.js # Painel de configuraÃ§Ãµes
â”‚   â”œâ”€â”€ ui-manager.js     # Gerenciamento da UI
â”‚   â””â”€â”€ utils.js          # UtilitÃ¡rios
â”œâ”€â”€ icons/                 # Ãcones da aplicaÃ§Ã£o
â”œâ”€â”€ backend/              # Backend Node.js (opcional)
â”‚   â”œâ”€â”€ server.js         # Servidor principal
â”‚   â”œâ”€â”€ services/         # ServiÃ§os do backend
â”‚   â”œâ”€â”€ middleware/       # Middlewares
â”‚   â”œâ”€â”€ .env.example      # Exemplo de configuraÃ§Ã£o
â”‚   â””â”€â”€ package.json      # DependÃªncias do backend
â””â”€â”€ README.md             # Este arquivo
```

## â— SoluÃ§Ã£o de Problemas

### Backend nÃ£o inicia
```bash
# Verifique se as dependÃªncias estÃ£o instaladas
cd backend && npm install

# Verifique se a porta estÃ¡ disponÃ­vel
netstat -an | findstr :3001

# Verifique os logs
npm start
```

### Ollama nÃ£o conecta
```bash
# Verifique se o Ollama estÃ¡ rodando
curl http://localhost:11434/api/tags

# Reinicie o serviÃ§o Ollama
ollama serve
```

### Upload de arquivos nÃ£o funciona
- Certifique-se de que o backend estÃ¡ rodando na porta 3001
- Verifique os tipos de arquivo permitidos no `.env`
- Confirme o limite de tamanho dos arquivos

### Erro de CORS
- Certifique-se de que o backend estÃ¡ configurado corretamente
- Verifique se as origens permitidas estÃ£o corretas

## ğŸ”„ AtualizaÃ§Ãµes

Para atualizar o projeto:

```bash
# Atualize as dependÃªncias do backend
cd backend
npm update

# Limpe o cache se necessÃ¡rio
npm cache clean --force
```

## ğŸ“ Suporte

Se vocÃª encontrar problemas:

1. Verifique este README primeiro
2. Consulte os logs do console do navegador (F12)
3. Verifique os logs do backend (terminal onde npm start foi executado)
4. Certifique-se de que todas as dependÃªncias estÃ£o instaladas
5. Verifique se as portas necessÃ¡rias estÃ£o disponÃ­veis

## ğŸ¯ PrÃ³ximos Passos

ApÃ³s a instalaÃ§Ã£o bem-sucedida:

1. **Configure seus providers favoritos** na seÃ§Ã£o "ConfiguraÃ§Ãµes"
2. **Experimente diferentes contextos** (EducaÃ§Ã£o Geral, Teoria Musical, Pesquisa)
3. **Teste upload de diferentes tipos de arquivo**
4. **Explore as funcionalidades de busca na web**
5. **Personalize a interface** conforme suas necessidades

---

**Darcy AI** - Seu tutor educacional inteligente e personalizado! ğŸš€
