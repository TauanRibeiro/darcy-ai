// Darcy AI - Backend Server with Modular LLM Support
// Supports multiple free LLM providers with automatic selection and fallback

const express = require('express');
const cors = require('cors');
const http = require('http');
const ModularLLMProvider = require('./modular-llm-provider');

const app = express();
const port = process.env.PORT || 3000;

// --- Middleware ---
app.use(cors({
    origin: ['http://localhost:8000', 'https://darcy-ai-seven.vercel.app', 'https://darcy-ai-*.vercel.app'],
    credentials: true
}));
app.use(express.json({ limit: '10mb' }));

// --- Initialize Modular LLM System ---
const llmProvider = new ModularLLMProvider();

// --- Enhanced Educational Content ---
const educationalContent = {
    'teaching': {
        name: 'Equipe de Ensino',
        description: 'Especializada em explicaÃ§Ãµes didÃ¡ticas e estruturadas',
        agents: ['ðŸ‘¨â€ðŸ« Professor Principal', 'ðŸ“ Criador de Exemplos', 'ðŸŽ¯ Facilitador de Aprendizado'],
        specialization: 'Ensino estruturado e didÃ¡tico'
    },
    'research': {
        name: 'Equipe de Pesquisa', 
        description: 'Focada em anÃ¡lise acadÃªmica e informaÃ§Ãµes verificÃ¡veis',
        agents: ['ðŸ” Pesquisador Senior', 'ðŸ“Š Analista de Dados', 'âœ… Verificador de Fontes'],
        specialization: 'Pesquisa acadÃªmica e anÃ¡lise'
    },
    'creative': {
        name: 'Equipe Criativa',
        description: 'Desenvolve projetos inovadores e atividades envolventes', 
        agents: ['ðŸŽ¨ Designer Educacional', 'ðŸ’¡ Inovador PedagÃ³gico', 'ðŸ› ï¸ Desenvolvedor de Projetos'],
        specialization: 'Criatividade e inovaÃ§Ã£o pedagÃ³gica'
    },
    'assessment': {
        name: 'Equipe de AvaliaÃ§Ã£o',
        description: 'Cria avaliaÃ§Ãµes construtivas e feedback personalizado',
        agents: ['ðŸ“‹ Especialista em AvaliaÃ§Ã£o', 'âœï¸ Criador de ExercÃ­cios', 'ðŸ“ˆ Analista de Progresso'],
        specialization: 'AvaliaÃ§Ã£o e feedback educacional'
    }
};

// --- API Routes ---

app.get('/', (req, res) => {
    const systemStatus = llmProvider.getSystemStatus();
    
    res.json({
        status: 'âœ… Darcy AI Backend Modular estÃ¡ funcionando!',
        version: '2.1.0',
        timestamp: new Date().toISOString(),
        llmSystem: {
            currentProvider: systemStatus.currentProvider,
            healthyProviders: systemStatus.healthyProviders,
            totalProviders: systemStatus.totalProviders,
            lastHealthCheck: new Date(systemStatus.lastHealthCheck).toISOString()
        },
        features: {
            'ðŸ¤– Sistema Modular LLM': true,
            'ðŸ”„ Fallback AutomÃ¡tico': true,
            'ðŸ©º Health Check': true,
            'ðŸ‘¥ Equipes Especializadas': true,
            'ðŸ†“ 100% Gratuito': true
        },
        crews: Object.keys(educationalContent),
        endpoints: [
            'GET / (status)',
            'POST /api/index (Vercel)',
            'POST /api/v1/crew/execute',
            'GET /api/status',
            'GET /api/health',
            'GET /api/providers'
        ]
    });
});

// Main endpoint for Vercel
app.post('/api/index', async (req, res) => {
    await handleCrewRequest(req, res);
});

// Alternative endpoint
app.post('/api/v1/crew/execute', async (req, res) => {
    await handleCrewRequest(req, res);
});

// Enhanced crew request handler with modular LLM support
async function handleCrewRequest(req, res) {
    const { mode, query, options = {} } = req.body;
    const startTime = Date.now();

    // Validate input
    if (!mode || !query) {
        return res.status(400).json({ 
            error: 'ParÃ¢metros obrigatÃ³rios: mode e query',
            example: { 
                mode: 'teaching', 
                query: 'Explique fÃ­sica quÃ¢ntica',
                options: { complexity: 'medium', format: 'structured' }
            },
            availableModes: Object.keys(educationalContent)
        });
    }

    // Validate crew mode
    if (!educationalContent[mode]) {
        return res.status(400).json({ 
            error: `Modo "${mode}" nÃ£o encontrado`,
            availableModes: Object.keys(educationalContent),
            suggestion: `VocÃª quis dizer "${findClosestMode(mode)}"?`
        });
    }

    const crewData = educationalContent[mode];
    
    try {
        console.log(`ðŸŽ“ [${crewData.name}] Processando: "${query.substring(0, 50)}${query.length > 50 ? '...' : ''}"`);
        
        // Generate response using modular LLM system
        const result = await llmProvider.generateResponse(mode, query, options);
        const processingTime = Date.now() - startTime;
        
        // Get current system status
        const systemStatus = llmProvider.getSystemStatus();
        
        res.json({ 
            result,
            metadata: {
                crew: crewData.name,
                specialization: crewData.specialization,
                query: query,
                mode: mode,
                processingTime: `${processingTime}ms`,
                timestamp: new Date().toISOString(),
                agents: crewData.agents,
                llmProvider: systemStatus.currentProvider,
                healthyProviders: systemStatus.healthyProviders
            },
            system: {
                version: '2.1.0',
                modularLLM: true,
                fallbackEnabled: true
            }
        });

        console.log(`âœ… Resposta gerada em ${processingTime}ms usando ${systemStatus.currentProvider}`);

    } catch (error) {
        const processingTime = Date.now() - startTime;
        console.error(`âŒ Erro ao processar solicitaÃ§Ã£o [${processingTime}ms]:`, error.message);
        
        res.status(500).json({ 
            error: 'Erro interno do servidor',
            message: 'Todos os provedores LLM estÃ£o temporariamente indisponÃ­veis',
            details: error.message,
            processingTime: `${processingTime}ms`,
            timestamp: new Date().toISOString(),
            suggestion: 'Tente novamente em alguns segundos. O sistema tentarÃ¡ outros provedores automaticamente.',
            fallbackAvailable: true
        });
    }
}

// Enhanced status endpoint with detailed LLM information
app.get('/api/status', (req, res) => {
    const systemStatus = llmProvider.getSystemStatus();
    
    res.json({
        status: 'operacional',
        version: '2.1.0',
        timestamp: new Date().toISOString(),
        uptime: `${Math.floor(process.uptime())}s`,
        llmSystem: {
            currentProvider: systemStatus.currentProvider,
            providersStatus: systemStatus.providersStatus,
            healthyProviders: systemStatus.healthyProviders,
            totalProviders: systemStatus.totalProviders,
            lastHealthCheck: new Date(systemStatus.lastHealthCheck).toISOString()
        },
        crews: Object.keys(educationalContent).map(key => ({
            mode: key,
            name: educationalContent[key].name,
            description: educationalContent[key].description,
            agents: educationalContent[key].agents.length,
            specialization: educationalContent[key].specialization
        })),
        capabilities: {
            'modular_llm': true,
            'auto_fallback': true,
            'health_monitoring': true,
            'multi_provider': true,
            'free_operation': true,
            'educational_focus': true
        },
        performance: {
            memoryUsage: process.memoryUsage(),
            nodeVersion: process.version
        }
    });
});

// Providers information endpoint
app.get('/api/providers', (req, res) => {
    const systemStatus = llmProvider.getSystemStatus();
    
    res.json({
        current: systemStatus.currentProvider,
        providers: systemStatus.providersStatus,
        summary: {
            total: systemStatus.totalProviders,
            healthy: systemStatus.healthyProviders,
            unhealthy: systemStatus.totalProviders - systemStatus.healthyProviders,
            lastCheck: new Date(systemStatus.lastHealthCheck).toISOString()
        },
        recommendations: getProviderRecommendations(systemStatus)
    });
});

// Health check for Vercel and monitoring
app.get('/api/health', (req, res) => {
    const systemStatus = llmProvider.getSystemStatus();
    const isHealthy = systemStatus.healthyProviders > 0 || systemStatus.currentProvider === 'SimulaÃ§Ã£o';
    
    res.status(isHealthy ? 200 : 503).json({ 
        status: isHealthy ? 'healthy' : 'degraded',
        timestamp: new Date().toISOString(),
        uptime: process.uptime(),
        providers: systemStatus.healthyProviders,
        fallbackAvailable: true
    });
});

// Utility functions
function findClosestMode(input) {
    const modes = Object.keys(educationalContent);
    const distances = modes.map(mode => ({
        mode,
        distance: levenshteinDistance(input.toLowerCase(), mode.toLowerCase())
    }));
    
    distances.sort((a, b) => a.distance - b.distance);
    return distances[0].mode;
}

function levenshteinDistance(str1, str2) {
    const matrix = [];
    
    for (let i = 0; i <= str2.length; i++) {
        matrix[i] = [i];
    }
    
    for (let j = 0; j <= str1.length; j++) {
        matrix[0][j] = j;
    }
    
    for (let i = 1; i <= str2.length; i++) {
        for (let j = 1; j <= str1.length; j++) {
            if (str2.charAt(i - 1) === str1.charAt(j - 1)) {
                matrix[i][j] = matrix[i - 1][j - 1];
            } else {
                matrix[i][j] = Math.min(
                    matrix[i - 1][j - 1] + 1,
                    matrix[i][j - 1] + 1,
                    matrix[i - 1][j] + 1
                );
            }
        }
    }
    
    return matrix[str2.length][str1.length];
}

function getProviderRecommendations(systemStatus) {
    const recommendations = [];
    
    if (systemStatus.healthyProviders === 0) {
        recommendations.push('âš ï¸ Nenhum provedor LLM disponÃ­vel. Instale o Ollama para melhor experiÃªncia.');
        recommendations.push('ðŸ’¡ Execute: curl -fsSL https://ollama.com/install.sh | sh');
    }
    
    if (systemStatus.currentProvider === 'SimulaÃ§Ã£o') {
        recommendations.push('ðŸ¤– Usando modo simulaÃ§Ã£o. Configure provedores LLM para respostas mais avanÃ§adas.');
    }
    
    const ollamaStatus = systemStatus.providersStatus['Ollama Local'];
    if (ollamaStatus && ollamaStatus.status !== 'healthy') {
        recommendations.push('ðŸ”§ Ollama local nÃ£o estÃ¡ funcionando. Verifique se estÃ¡ instalado e rodando.');
    }
    
    return recommendations;
}

// --- Server ---
const server = http.createServer(app);

if (require.main === module) {
    server.listen(port, () => {
        console.log('');
        console.log('ðŸš€ ================================');
        console.log('ðŸ¤– Darcy AI Backend Modular v2.1.0');
        console.log('ðŸš€ ================================');
        console.log(`ðŸ“¡ Servidor: http://localhost:${port}`);
        console.log('ðŸ”§ Sistema LLM: Modular com fallback automÃ¡tico');
        console.log('ðŸ†“ Modo: 100% gratuito');
        console.log('');
        console.log('ðŸ“‹ Endpoints disponÃ­veis:');
        console.log('  - GET  / (status do sistema)');
        console.log('  - POST /api/index (Vercel)');
        console.log('  - POST /api/v1/crew/execute (crews)');
        console.log('  - GET  /api/status (status detalhado)');
        console.log('  - GET  /api/providers (provedores LLM)');
        console.log('  - GET  /api/health (health check)');
        console.log('');
        console.log('ðŸŽ“ Equipes disponÃ­veis:');
        Object.entries(educationalContent).forEach(([key, crew]) => {
            console.log(`  - ${key}: ${crew.name} (${crew.specialization})`);
        });
        console.log('');
        console.log('ðŸ’¡ Provedores LLM suportados:');
        console.log('  - ðŸ  Ollama Local (llama3.1, mistral, codellama)');
        console.log('  - âš¡ Groq API (gratuito com limitaÃ§Ãµes)'); 
        console.log('  - ðŸ¤— Hugging Face (modelos gratuitos)');
        console.log('  - ðŸ”¬ Together AI (crÃ©ditos gratuitos)');
        console.log('  - ðŸ§  Cohere (trial gratuito)');
        console.log('  - ðŸ” Perplexity (uso gratuito limitado)');
        console.log('');
        console.log('âœ¨ Sistema iniciado com sucesso!');
        console.log('');
    });
}

module.exports = app;

// Graceful shutdown
process.on('SIGTERM', () => {
    console.log('ðŸ”„ Encerrando servidor...');
    server.close(() => {
        console.log('âœ… Servidor encerrado graciosamente');
        process.exit(0);
    });
});

process.on('SIGINT', () => {
    console.log('\nðŸ”„ Encerrando servidor...');
    server.close(() => {
        console.log('âœ… Servidor encerrado');
        process.exit(0);
    });
});
